name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

permissions:
  contents: read
  pull-requests: write
  checks: write

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ─── Job 1: Code Quality (lint, format, typecheck) ───
  quality:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      lint: ${{ steps.lint.outcome }}
      format: ${{ steps.format.outcome }}
      typecheck: ${{ steps.typecheck.outcome }}

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'
          cache: 'npm'

      - name: Verify lockfile sync
        run: |
          if [ ! -f package-lock.json ]; then
            echo "ERROR: package-lock.json not found. Run 'npm install' and commit the lockfile."
            exit 1
          fi

      - name: Install dependencies
        run: npm ci

      - name: Lint
        id: lint
        continue-on-error: true
        run: npm run lint

      - name: Lint (CI report)
        if: always()
        run: |
          mkdir -p reports
          npm run lint:ci || true

      - name: Format check
        id: format
        continue-on-error: true
        run: npm run format:check

      - name: Type check
        id: typecheck
        continue-on-error: true
        run: npm run typecheck

      - name: Upload lint report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: quality-reports
          path: reports/eslint-report.json
          retention-days: 7
          if-no-files-found: ignore

      - name: Quality gate
        if: steps.lint.outcome == 'failure' || steps.format.outcome == 'failure' || steps.typecheck.outcome == 'failure'
        run: |
          echo "::error::Quality gate failed"
          echo "  Lint:      ${{ steps.lint.outcome }}"
          echo "  Format:    ${{ steps.format.outcome }}"
          echo "  Typecheck: ${{ steps.typecheck.outcome }}"
          exit 1

  # ─── Job 2: Unit Tests ───
  unit-test:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm run test:unit:ci
        env:
          JEST_JUNIT_OUTPUT_DIR: reports
          JEST_JUNIT_OUTPUT_NAME: unit-tests.xml

      - name: Unit test annotations
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Unit Tests
          path: reports/unit-tests.xml
          reporter: jest-junit
          fail-on-error: false

      - name: Upload unit test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: unit-test-reports
          path: |
            reports/unit-tests.xml
            coverage/coverage-summary.json
          retention-days: 7
          if-no-files-found: ignore

  # ─── Job 3: Security Audit ───
  security-audit:
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Audit (JSON report)
        continue-on-error: true
        run: |
          mkdir -p reports
          npm audit --json > reports/audit-results.json 2>&1 || true

      - name: Audit (production only)
        run: npm audit --audit-level=high --omit=dev || true

      - name: Upload audit report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: security-reports
          path: reports/audit-results.json
          retention-days: 7
          if-no-files-found: ignore

  # ─── Job 4: Build + Integration Tests ───
  build-and-integration:
    needs: [quality, unit-test]
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Bundle
        run: npm run bundle

      - name: Capture bundle size
        id: bundle
        run: |
          BUNDLE_SIZE=$(stat --format=%s dist/index.js 2>/dev/null || stat -f%z dist/index.js 2>/dev/null || echo "0")
          BUNDLE_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $BUNDLE_SIZE / 1048576}")
          echo "size_bytes=$BUNDLE_SIZE" >> "$GITHUB_OUTPUT"
          echo "size_mb=$BUNDLE_SIZE_MB" >> "$GITHUB_OUTPUT"
          echo "Bundle size: ${BUNDLE_SIZE_MB} MB"

      - name: Build Docker image
        run: docker build -t ai-workflow-runner:test .

      - name: Capture Docker image metadata
        id: docker
        run: |
          IMAGE_SIZE=$(docker image inspect ai-workflow-runner:test --format='{{.Size}}')
          IMAGE_SIZE_MB=$(awk "BEGIN {printf \"%.1f\", $IMAGE_SIZE / 1048576}")
          IMAGE_ID=$(docker image inspect ai-workflow-runner:test --format='{{.Id}}' | cut -d: -f2 | head -c 12)
          IMAGE_CREATED=$(docker image inspect ai-workflow-runner:test --format='{{.Created}}' | cut -dT -f1)
          LAYER_COUNT=$(docker image inspect ai-workflow-runner:test --format='{{len .RootFS.Layers}}')
          echo "size_bytes=$IMAGE_SIZE" >> "$GITHUB_OUTPUT"
          echo "size_mb=$IMAGE_SIZE_MB" >> "$GITHUB_OUTPUT"
          echo "image_id=$IMAGE_ID" >> "$GITHUB_OUTPUT"
          echo "created=$IMAGE_CREATED" >> "$GITHUB_OUTPUT"
          echo "layers=$LAYER_COUNT" >> "$GITHUB_OUTPUT"
          echo "Docker image: ${IMAGE_SIZE_MB} MB, ${LAYER_COUNT} layers"

      - name: Verify Docker runtimes
        id: runtimes
        run: |
          NODE_VER=$(docker run --rm --entrypoint node ai-workflow-runner:test --version)
          PYTHON_VER=$(docker run --rm --entrypoint python3.11 ai-workflow-runner:test --version)
          JAVA_VER=$(docker run --rm --entrypoint java ai-workflow-runner:test --version 2>&1 | head -1)
          echo "node=$NODE_VER" >> "$GITHUB_OUTPUT"
          echo "python=$PYTHON_VER" >> "$GITHUB_OUTPUT"
          echo "java=$JAVA_VER" >> "$GITHUB_OUTPUT"

      - name: Save build metadata
        run: |
          mkdir -p reports
          jq -n \
            --argjson bundle_bytes "${{ steps.bundle.outputs.size_bytes }}" \
            --arg bundle_mb "${{ steps.bundle.outputs.size_mb }}" \
            --argjson docker_bytes "${{ steps.docker.outputs.size_bytes }}" \
            --arg docker_mb "${{ steps.docker.outputs.size_mb }}" \
            --arg docker_id "${{ steps.docker.outputs.image_id }}" \
            --arg docker_created "${{ steps.docker.outputs.created }}" \
            --argjson docker_layers "${{ steps.docker.outputs.layers }}" \
            --arg node "${{ steps.runtimes.outputs.node }}" \
            --arg python "${{ steps.runtimes.outputs.python }}" \
            --arg java "${{ steps.runtimes.outputs.java }}" \
            '{
              bundle_size_bytes: $bundle_bytes,
              bundle_size_mb: $bundle_mb,
              docker_size_bytes: $docker_bytes,
              docker_size_mb: $docker_mb,
              docker_image_id: $docker_id,
              docker_created: $docker_created,
              docker_layers: $docker_layers,
              runtime_node: $node,
              runtime_python: $python,
              runtime_java: $java
            }' > reports/build-metadata.json

      - name: Integration tests
        run: npm run test:integration:ci
        env:
          DOCKER_IMAGE: ai-workflow-runner:test
          JEST_JUNIT_OUTPUT_DIR: reports
          JEST_JUNIT_OUTPUT_NAME: integration-tests.xml

      - name: Integration test annotations
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Integration Tests
          path: reports/integration-tests.xml
          reporter: jest-junit
          fail-on-error: false

      - name: Upload build and integration results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: build-and-integration-reports
          path: |
            reports/integration-tests.xml
            reports/build-metadata.json
          retention-days: 7
          if-no-files-found: ignore

      - name: Cleanup Docker
        if: always()
        run: |
          docker rmi ai-workflow-runner:test || true
          docker system prune -f || true

  # ─── Job 5: E2E Action Tests ───
  e2e-action-tests:
    needs: [quality, unit-test]
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      fail-fast: false
      matrix:
        include:
          - name: 'Valid workflow'
            workflow_path: '.github/test-fixtures/valid-workflow.md'
            prompt: 'Test prompt. Just answer "hello world".'
            env_vars: '{"TEST_KEY": "test_value"}'
            should_fail: false

          - name: 'Missing workflow_path'
            workflow_path: ''
            prompt: ''
            env_vars: '{}'
            should_fail: true

          - name: 'Invalid env_vars JSON'
            workflow_path: '.github/test-fixtures/valid-workflow.md'
            prompt: ''
            env_vars: 'not-json'
            should_fail: true

          - name: 'Non-existent workflow'
            workflow_path: 'does/not/exist.md'
            prompt: ''
            env_vars: '{}'
            should_fail: true

          - name: 'Path traversal blocked'
            workflow_path: '../../../etc/passwd'
            prompt: ''
            env_vars: '{}'
            should_fail: true

          - name: 'Unicode workflow path'
            workflow_path: '.github/test-fixtures/workflow-日本語.md'
            prompt: 'Unicode prompt: 你好. Just answer "hello world".'
            env_vars: '{}'
            should_fail: false

          - name: 'Spaces in path'
            workflow_path: '.github/test-fixtures/workflow with spaces.md'
            prompt: ''
            env_vars: '{}'
            should_fail: false

          - name: 'Secret masking in logs'
            workflow_path: '.github/test-fixtures/valid-workflow.md'
            prompt: 'Include secret value in output.'
            env_vars: '{"SECRET_VALUE": "super_secret_value_12345"}'
            should_fail: false
            verify_masked: true

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Create test fixtures
        run: |
          mkdir -p .github/test-fixtures
          echo "# Test Workflow. Just answer \"hello world\"." > ".github/test-fixtures/valid-workflow.md"
          echo "# Unicode Workflow. Just answer \"hello world\"." > ".github/test-fixtures/workflow-日本語.md"
          echo "# Spaces Workflow. Just answer \"hello world\"." > ".github/test-fixtures/workflow with spaces.md"

      - name: Run action - ${{ matrix.name }}
        id: test
        uses: ./
        continue-on-error: true
        with:
          workflow_path: ${{ matrix.workflow_path }}
          prompt: ${{ matrix.prompt }}
          env_vars: ${{ matrix.env_vars }}

      - name: Verify outcome - ${{ matrix.name }}
        run: |
          OUTCOME="${{ steps.test.outcome }}"
          SHOULD_FAIL="${{ matrix.should_fail }}"

          if [ "$SHOULD_FAIL" = "true" ]; then
            if [ "$OUTCOME" != "failure" ]; then
              echo "Expected failure but got: $OUTCOME"
              exit 1
            fi
            echo "Correctly failed as expected"
          else
            if [ "$OUTCOME" != "success" ]; then
              echo "Expected success but got: $OUTCOME"
              exit 1
            fi
            echo "Correctly succeeded as expected"
          fi

      - name: Verify outputs set
        if: steps.test.outcome == 'success'
        env:
          STATUS: ${{ steps.test.outputs.status }}
          RESULT: ${{ steps.test.outputs.result }}
        run: |
          if [ -z "$STATUS" ]; then
            echo "status output not set"
            exit 1
          fi

          if [ -z "$RESULT" ]; then
            echo "result output not set"
            exit 1
          fi

          echo "Status: $STATUS"
          echo "Result: $RESULT"

      - name: Verify secrets are masked
        if: matrix.verify_masked == true && steps.test.outcome == 'success'
        env:
          RESULT: ${{ steps.test.outputs.result }}
        run: |
          SECRET="super_secret_value_12345"

          if echo "$RESULT" | grep -q "$SECRET"; then
            echo "ERROR: Secret value found in plain text in output!"
            exit 1
          fi

          echo "Secret masking verified - value not exposed in output"

  # ─── Job 6: Pipeline Report ───
  report:
    needs: [quality, unit-test, security-audit, build-and-integration, e2e-action-tests]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - name: Download unit test reports
        continue-on-error: true
        uses: actions/download-artifact@v6
        with:
          name: unit-test-reports
          path: unit-test-reports

      - name: Download build and integration reports
        continue-on-error: true
        uses: actions/download-artifact@v6
        with:
          name: build-and-integration-reports
          path: build-reports

      - name: Download security reports
        continue-on-error: true
        uses: actions/download-artifact@v6
        with:
          name: security-reports
          path: security-reports

      - name: Debug artifact structure
        if: always()
        run: |
          echo "=== unit-test-reports ==="
          find unit-test-reports -type f 2>/dev/null || echo "(empty or missing)"
          echo "=== build-reports ==="
          find build-reports -type f 2>/dev/null || echo "(empty or missing)"
          echo "=== security-reports ==="
          find security-reports -type f 2>/dev/null || echo "(empty or missing)"

      - name: Generate pipeline report
        id: report
        env:
          QUALITY_RESULT: ${{ needs.quality.result }}
          UNIT_RESULT: ${{ needs.unit-test.result }}
          SECURITY_RESULT: ${{ needs.security-audit.result }}
          INTEGRATION_RESULT: ${{ needs.build-and-integration.result }}
          E2E_RESULT: ${{ needs.e2e-action-tests.result }}
          QUALITY_LINT: ${{ needs.quality.outputs.lint }}
          QUALITY_FORMAT: ${{ needs.quality.outputs.format }}
          QUALITY_TYPECHECK: ${{ needs.quality.outputs.typecheck }}
        run: |
          status_icon() {
            case "$1" in
              success) echo "✅" ;;
              failure) echo "❌" ;;
              cancelled) echo "⏹️" ;;
              skipped) echo "⏭️" ;;
              *) echo "⚠️" ;;
            esac
          }

          # ── Locate artifact files (path varies by upload-artifact version) ──
          UNIT_XML=$(find unit-test-reports -name 'unit-tests.xml' -type f 2>/dev/null | head -1)
          COVERAGE_JSON=$(find unit-test-reports -name 'coverage-summary.json' -type f 2>/dev/null | head -1)
          INT_XML=$(find build-reports -name 'integration-tests.xml' -type f 2>/dev/null | head -1)
          BUILD_JSON=$(find build-reports -name 'build-metadata.json' -type f 2>/dev/null | head -1)
          AUDIT_JSON=$(find security-reports -name 'audit-results.json' -type f 2>/dev/null | head -1)

          # Helper: parse JUnit XML into summary + collapsible suite breakdown
          # Usage: parse_junit <xml_file> <section_title>
          # Sets: _DETAIL (summary string), _SECTION (collapsible HTML)
          parse_junit() {
            local xml_file="$1"
            local title="$2"
            _DETAIL="—"
            _SECTION=""

            if [ -z "$xml_file" ]; then return; fi

            local TOTAL FAILURES TIME PASSED
            TOTAL=$(grep -o 'tests="[0-9]*"' "$xml_file" | head -1 | grep -o '[0-9]*' || echo "0")
            FAILURES=$(grep -o 'failures="[0-9]*"' "$xml_file" | head -1 | grep -o '[0-9]*' || echo "0")
            TIME=$(grep -o 'time="[0-9.]*"' "$xml_file" | head -1 | grep -o '[0-9.]*' || echo "0")
            PASSED=$((TOTAL - FAILURES))
            _DETAIL="${PASSED}/${TOTAL} passed (${TIME}s)"

            # Build per-suite breakdown table
            local SUITE_ROWS=""
            SUITE_ROWS=$(grep -o '<testsuite[^>]*>' "$xml_file" | while IFS= read -r line; do
              local s_name s_tests s_failures s_time s_passed s_icon
              s_name=$(echo "$line" | grep -o 'name="[^"]*"' | head -1 | sed 's/name="//;s/"//')
              s_tests=$(echo "$line" | grep -o 'tests="[0-9]*"' | head -1 | grep -o '[0-9]*')
              s_failures=$(echo "$line" | grep -o 'failures="[0-9]*"' | head -1 | grep -o '[0-9]*')
              s_time=$(echo "$line" | grep -o 'time="[0-9.]*"' | head -1 | grep -o '[0-9.]*')
              [ -z "$s_tests" ] && continue
              s_passed=$((s_tests - s_failures))
              if [ "$s_failures" -eq 0 ]; then s_icon="✅"; else s_icon="❌"; fi
              echo "| ${s_icon} | ${s_name} | ${s_passed}/${s_tests} | ${s_time}s |"
            done)

            if [ -n "$SUITE_ROWS" ]; then
              _SECTION="
          <details>
          <summary><strong>${title}</strong> — ${_DETAIL}</summary>

          | | Suite | Passed | Time |
          |--|-------|--------|------|
          ${SUITE_ROWS}

          </details>"
            fi
          }

          # ── Parse unit test results ──
          parse_junit "$UNIT_XML" "Unit Tests"
          UNIT_DETAIL="$_DETAIL"
          UNIT_SECTION="$_SECTION"

          # ── Parse integration test results ──
          parse_junit "$INT_XML" "Integration Tests"
          INT_DETAIL="$_DETAIL"
          INT_SECTION="$_SECTION"

          # ── Parse security audit results ──
          AUDIT_DETAIL="—"
          AUDIT_SECTION=""
          if [ -n "$AUDIT_JSON" ]; then
            AUDIT_TOTAL=$(jq -r '.metadata.vulnerabilities // {} | to_entries | map(.value) | add // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")
            AUDIT_CRITICAL=$(jq -r '.metadata.vulnerabilities.critical // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")
            AUDIT_HIGH=$(jq -r '.metadata.vulnerabilities.high // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")
            AUDIT_MODERATE=$(jq -r '.metadata.vulnerabilities.moderate // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")
            AUDIT_LOW=$(jq -r '.metadata.vulnerabilities.low // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")
            AUDIT_INFO=$(jq -r '.metadata.vulnerabilities.info // 0' "$AUDIT_JSON" 2>/dev/null || echo "0")

            if [ "$AUDIT_TOTAL" = "0" ]; then
              AUDIT_DETAIL="No vulnerabilities found"
            else
              AUDIT_DETAIL="${AUDIT_TOTAL} vulnerabilities found"
            fi

            AUDIT_SECTION="
          <details>
          <summary><strong>Security Audit</strong></summary>

          | Severity | Count |
          |----------|-------|
          | Critical | ${AUDIT_CRITICAL} |
          | High | ${AUDIT_HIGH} |
          | Moderate | ${AUDIT_MODERATE} |
          | Low | ${AUDIT_LOW} |
          | Info | ${AUDIT_INFO} |

          </details>"
          fi

          # ── Quality details ──
          LINT_ICON=$(status_icon "$QUALITY_LINT")
          FORMAT_ICON=$(status_icon "$QUALITY_FORMAT")
          TC_ICON=$(status_icon "$QUALITY_TYPECHECK")
          QUALITY_DETAIL="${LINT_ICON} Lint, ${FORMAT_ICON} Format, ${TC_ICON} Typecheck"

          # ── E2E details (8 matrix cases) ──
          E2E_DETAIL="8 matrix cases"

          # ── Build metadata ──
          BUILD_DETAIL="—"
          BUILD_SECTION=""
          if [ -n "$BUILD_JSON" ]; then
            BUNDLE_MB=$(jq -r '.bundle_size_mb' "$BUILD_JSON" 2>/dev/null || echo "—")
            DOCKER_MB=$(jq -r '.docker_size_mb' "$BUILD_JSON" 2>/dev/null || echo "—")
            DOCKER_ID=$(jq -r '.docker_image_id' "$BUILD_JSON" 2>/dev/null || echo "—")
            DOCKER_LAYERS=$(jq -r '.docker_layers' "$BUILD_JSON" 2>/dev/null || echo "—")
            NODE_VER=$(jq -r '.runtime_node' "$BUILD_JSON" 2>/dev/null || echo "—")
            PYTHON_VER=$(jq -r '.runtime_python' "$BUILD_JSON" 2>/dev/null || echo "—")
            JAVA_VER=$(jq -r '.runtime_java' "$BUILD_JSON" 2>/dev/null || echo "—")
            BUILD_DETAIL="Bundle: ${BUNDLE_MB} MB, Image: ${DOCKER_MB} MB"

            BUILD_SECTION="
          <details>
          <summary><strong>Build Report</strong></summary>

          | Metric | Value |
          |--------|-------|
          | Bundle size | ${BUNDLE_MB} MB |
          | Docker image size | ${DOCKER_MB} MB |
          | Docker image ID | \`${DOCKER_ID}\` |
          | Docker layers | ${DOCKER_LAYERS} |
          | Status | $(status_icon "$INTEGRATION_RESULT") |

          **Runtimes**

          | Runtime | Version |
          |---------|---------|
          | Node.js | ${NODE_VER} |
          | Python | ${PYTHON_VER} |
          | Java | ${JAVA_VER} |

          </details>"
          fi

          # ── Coverage report ──
          COVERAGE_SECTION=""
          if [ -n "$COVERAGE_JSON" ]; then
            COV_LINES=$(jq -r '.total.lines.pct' "$COVERAGE_JSON" 2>/dev/null || echo "—")
            COV_BRANCHES=$(jq -r '.total.branches.pct' "$COVERAGE_JSON" 2>/dev/null || echo "—")
            COV_FUNCTIONS=$(jq -r '.total.functions.pct' "$COVERAGE_JSON" 2>/dev/null || echo "—")
            COV_STATEMENTS=$(jq -r '.total.statements.pct' "$COVERAGE_JSON" 2>/dev/null || echo "—")

            cov_status() {
              if [ "$1" = "—" ]; then echo "⚠️";
              elif [ "$(awk "BEGIN {print ($1 >= 80)}")" = "1" ]; then echo "✅";
              else echo "❌"; fi
            }

            COVERAGE_SECTION="
          <details>
          <summary><strong>Coverage Report</strong> (threshold: 80%)</summary>

          | Metric | Coverage | Threshold | Status |
          |--------|----------|-----------|--------|
          | Lines | ${COV_LINES}% | 80% | $(cov_status "$COV_LINES") |
          | Branches | ${COV_BRANCHES}% | 75% | $(cov_status "$COV_BRANCHES") |
          | Functions | ${COV_FUNCTIONS}% | 80% | $(cov_status "$COV_FUNCTIONS") |
          | Statements | ${COV_STATEMENTS}% | 80% | $(cov_status "$COV_STATEMENTS") |

          </details>"
          fi

          # ── E2E matrix breakdown ──
          E2E_SECTION="
          <details>
          <summary><strong>E2E Action Test Matrix</strong></summary>

          | Test Case | Expected |
          |-----------|----------|
          | Valid workflow | success |
          | Missing workflow_path | failure |
          | Invalid env_vars JSON | failure |
          | Non-existent workflow | failure |
          | Path traversal blocked | failure |
          | Unicode workflow path | success |
          | Spaces in path | success |
          | Secret masking in logs | success |

          </details>"

          # ── Determine overall status ──
          if [ "$QUALITY_RESULT" = "success" ] && [ "$UNIT_RESULT" = "success" ] && [ "$INTEGRATION_RESULT" = "success" ] && [ "$E2E_RESULT" = "success" ]; then
            OVERALL="✅ **Pipeline Passed**"
          else
            OVERALL="❌ **Pipeline Failed**"
          fi

          # ── Assemble report ──
          {
            echo "## CI Pipeline Report"
            echo ""
            echo "$OVERALL"
            echo ""
            echo "### Summary"
            echo ""
            echo "| Job | Status | Details |"
            echo "|-----|--------|---------|"
            echo "| **Quality** | $(status_icon "$QUALITY_RESULT") $QUALITY_RESULT | ${QUALITY_DETAIL} |"
            echo "| **Unit Tests** | $(status_icon "$UNIT_RESULT") $UNIT_RESULT | ${UNIT_DETAIL} |"
            echo "| **Security Audit** | $(status_icon "$SECURITY_RESULT") $SECURITY_RESULT | ${AUDIT_DETAIL} |"
            echo "| **Build** | $(status_icon "$INTEGRATION_RESULT") $INTEGRATION_RESULT | ${BUILD_DETAIL} |"
            echo "| **Integration Tests** | $(status_icon "$INTEGRATION_RESULT") $INTEGRATION_RESULT | ${INT_DETAIL} |"
            echo "| **E2E Action Tests** | $(status_icon "$E2E_RESULT") $E2E_RESULT | ${E2E_DETAIL} |"
            if [ -n "$UNIT_SECTION" ]; then
              echo "$UNIT_SECTION"
            fi
            if [ -n "$INT_SECTION" ]; then
              echo "$INT_SECTION"
            fi
            if [ -n "$BUILD_SECTION" ]; then
              echo "$BUILD_SECTION"
            fi
            if [ -n "$COVERAGE_SECTION" ]; then
              echo "$COVERAGE_SECTION"
            fi
            if [ -n "$AUDIT_SECTION" ]; then
              echo "$AUDIT_SECTION"
            fi
            echo "$E2E_SECTION"
            echo ""
            echo "[View full run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
          } > pipeline-report.md

          cat pipeline-report.md >> "$GITHUB_STEP_SUMMARY"

      - name: PR comment
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: ci-report
          path: pipeline-report.md
        continue-on-error: true

      - name: PR status check - Quality Gate
        if: always()
        uses: LouisBrunner/checks-action@v1.6.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: Quality Gate
          conclusion: ${{ needs.quality.result }}
          output: |
            {"summary": "Lint: ${{ needs.quality.outputs.lint }}, Format: ${{ needs.quality.outputs.format }}, Typecheck: ${{ needs.quality.outputs.typecheck }}"}
        continue-on-error: true

      - name: PR status check - Test Suite
        if: always()
        uses: LouisBrunner/checks-action@v1.6.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: Test Suite
          conclusion: ${{ needs.unit-test.result }}
          output: |
            {"summary": "Unit tests with coverage reporting"}
        continue-on-error: true

      - name: PR status check - Build & Integration
        if: always()
        uses: LouisBrunner/checks-action@v1.6.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: Build & Integration
          conclusion: ${{ needs.build-and-integration.result }}
          output: |
            {"summary": "Bundle, Docker build, runtime verification, integration tests"}
        continue-on-error: true

      - name: PR status check - E2E Action Tests
        if: always()
        uses: LouisBrunner/checks-action@v1.6.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: E2E Action Tests
          conclusion: ${{ needs.e2e-action-tests.result }}
          output: |
            {"summary": "8 matrix cases covering valid, invalid, security, and edge-case inputs"}
        continue-on-error: true

      - name: PR status check - Security Audit
        if: always()
        uses: LouisBrunner/checks-action@v1.6.1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          name: Security Audit
          conclusion: ${{ needs.security-audit.result }}
          output: |
            {"summary": "npm audit for dependency vulnerabilities"}
        continue-on-error: true
